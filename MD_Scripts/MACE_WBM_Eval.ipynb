{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "from matsciml.datasets.transforms import (\n",
    "    PeriodicPropertiesTransform,\n",
    "    PointCloudToGraphTransform,\n",
    "    FrameAveraging,\n",
    ")\n",
    "\n",
    "from matsciml.lightning.data_utils import MatSciMLDataModule\n",
    "from matsciml.models.base import ScalarRegressionTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:199: Attribute 'gate' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['gate'])`.\n",
      "No ``atomic_energies`` provided, defaulting to ones.\n",
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:199: Attribute 'gate' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['gate'])`.\n",
      "/home/m3rg2000/Simulation/Eval_NVT_Env/matsciml/matsciml/models/base.py:1979: UserWarning: GradFreeForceRegressionTask does not `task_keys`; ignoring passed keys: ['force']\n",
      "  warn(\n",
      "No ``atomic_energies`` provided, defaulting to ones.\n",
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/m3rg2000/miniconda3/envs/matsciml/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:199: Attribute 'gate' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['gate'])`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiTask Training module:\n",
       "MaterialsProjectDataset-regression0\n",
       "MaterialsProjectDataset-gff_regression0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matsciml.models.utils.io import * \n",
    "checkpoint_path = \"/home/m3rg2000/Simulation/checkpoints-2024/mace_mat_traj_mar26_24.ckpt\"\n",
    "task = multitask_from_checkpoint(checkpoint_path)\n",
    "task.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_factors={\n",
    "    \"energy_mean\": -214.69845189724998,\n",
    "    \"energy_std\": 218.69620660947086,\n",
    "    \"corrected_total_energy_mean\": -214.69845189724998,\n",
    "    \"corrected_total_energy_std\": 218.69620660947086\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "dm = MatSciMLDataModule(\n",
    "    \"MaterialsProjectDataset\",\n",
    "    train_path=\"/home/m3rg2000/matsciml/Scale_new_lmdb/10k_new\",#TRAIN_PATH,\n",
    "    # val_split=VAL_PATH,\n",
    "    # test_split=VAL_PATH,\n",
    "    dset_kwargs={\n",
    "        \"transforms\": [\n",
    "            PeriodicPropertiesTransform(cutoff_radius=6.0, adaptive_cutoff=True),\n",
    "            PointCloudToGraphTransform(\n",
    "                \"pyg\",\n",
    "                node_keys=[\"pos\", \"atomic_numbers\"],\n",
    "            ),\n",
    "        ],\n",
    "    },\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "dm.setup()\n",
    "train_loader = dm.train_dataloader()\n",
    "dataset_iter = iter(train_loader)\n",
    "batch = next(dataset_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, Batch\n",
    "# def batch2cuda(batch):\n",
    "#     for key in batch.keys():\n",
    "#         if type(batch[key])==torch.Tensor:\n",
    "#             batch[key]=batch[key].to('cuda')\n",
    "#     batch['graph']=batch['graph'].to('cuda')\n",
    "def batch2cuda(batch):\n",
    "    # Recursively move all tensors in nested structures to 'cuda'\n",
    "    # batch['graph']=batch['graph'].to('cuda')\n",
    "    if isinstance(batch, dict):\n",
    "        return {k: batch2cuda(v) for k, v in batch.items()}\n",
    "    elif isinstance(batch, list):\n",
    "        return [batch2cuda(v) for v in batch]\n",
    "    elif isinstance(batch, torch.Tensor):\n",
    "        return batch.to('cuda')\n",
    "    elif isinstance(batch, Data) or isinstance(batch, Batch):\n",
    "        return batch.to('cuda')\n",
    "    else:\n",
    "        return batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"<eval_with_key>.21\", line 49, in forward\n    einsum_3 = torch.functional.einsum('edb,eca->ecdab', reshape_5, reshape_3)\n    reshape_12 = reshape_3.reshape(getitem_4, 64)\n    einsum_4 = torch.functional.einsum('ca,cab->cab', reshape_12, reshape_11);  reshape_12 = reshape_11 = None\n               ~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    einsum_5 = torch.functional.einsum('dbc,dca->dba', einsum_4, reshape_5);  einsum_4 = reshape_5 = None\n    reshape_13 = einsum_5.reshape(getitem_4, 192);  einsum_5 = None\nRuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.75 GiB of which 15.12 MiB is free. Process 392269 has 174.00 MiB memory in use. Process 2007541 has 10.59 GiB memory in use. Process 1709378 has 150.00 MiB memory in use. Process 3487595 has 554.00 MiB memory in use. Including non-PyTorch memory, this process has 222.00 MiB memory in use. Of the allocated memory 82.87 MiB is allocated by PyTorch, and 17.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      5\u001b[0m     batch2cuda(batch)\n\u001b[0;32m----> 6\u001b[0m     Result\u001b[38;5;241m=\u001b[39m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     Pred_Energy\u001b[38;5;241m=\u001b[39mResult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression0\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrected_total_energy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m*\u001b[39mnorm_factors[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menergy_std\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39mnorm_factors[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menergy_mean\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m     Pred_Forces\u001b[38;5;241m=\u001b[39mResult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgff_regression0\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforce\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Simulation/Eval_NVT_Env/matsciml/matsciml/models/base.py:2596\u001b[0m, in \u001b[0;36mMultiTaskLitModule.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m   2594\u001b[0m         data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(data)\n\u001b[1;32m   2595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2596\u001b[0m     batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2597\u001b[0m \u001b[38;5;66;03m# for single dataset usage, we assume the nested structure isn't used\u001b[39;00m\n\u001b[1;32m   2598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_multidata:\n",
      "File \u001b[0;32m~/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Simulation/Eval_NVT_Env/matsciml/matsciml/models/base.py:285\u001b[0m, in \u001b[0;36mAbstractTask.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03mGiven a batch structure, extract out data and pass it into the\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03mneural network architecture. This implements the 'forward' method\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m    Data structure containing system/graph and point/node level embeddings.\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    284\u001b[0m input_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_batch(batch)\n\u001b[0;32m--> 285\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# raise an error to help spot models that have not yet been refactored\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, Embeddings):\n",
      "File \u001b[0;32m~/Simulation/Eval_NVT_Env/matsciml/matsciml/models/pyg/mace/wrapper/model.py:171\u001b[0m, in \u001b[0;36mMACEWrapper._forward\u001b[0;34m(self, graph, node_feats, pos, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# repack data into MACE format\u001b[39;00m\n\u001b[1;32m    162\u001b[0m mace_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositions\u001b[39m\u001b[38;5;124m\"\u001b[39m: pos,\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_attrs\u001b[39m\u001b[38;5;124m\"\u001b[39m: node_feats,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m\"\u001b[39m: graph\u001b[38;5;241m.\u001b[39medge_index,\n\u001b[1;32m    170\u001b[0m }\n\u001b[0;32m--> 171\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmace_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_force\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_virials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_stress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_displacement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m node_embeddings \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_feats\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    180\u001b[0m graph_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreadout(node_embeddings, graph\u001b[38;5;241m.\u001b[39mbatch)\n",
      "File \u001b[0;32m~/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/matsciml/lib/python3.9/site-packages/mace/modules/models.py:217\u001b[0m, in \u001b[0;36mMACE.forward\u001b[0;34m(self, data, training, compute_force, compute_virials, compute_stress, compute_displacement)\u001b[0m\n\u001b[1;32m    213\u001b[0m node_feats_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m interaction, product, readout \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minteractions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproducts, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreadouts\n\u001b[1;32m    216\u001b[0m ):\n\u001b[0;32m--> 217\u001b[0m     node_feats, sc \u001b[38;5;241m=\u001b[39m \u001b[43minteraction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnode_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnode_attrs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnode_feats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_feats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_feats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_feats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43medge_index\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m     node_feats \u001b[38;5;241m=\u001b[39m product(\n\u001b[1;32m    225\u001b[0m         node_feats\u001b[38;5;241m=\u001b[39mnode_feats,\n\u001b[1;32m    226\u001b[0m         sc\u001b[38;5;241m=\u001b[39msc,\n\u001b[1;32m    227\u001b[0m         node_attrs\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_attrs\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    228\u001b[0m     )\n\u001b[1;32m    229\u001b[0m     node_feats_list\u001b[38;5;241m.\u001b[39mappend(node_feats)\n",
      "File \u001b[0;32m~/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/matsciml/lib/python3.9/site-packages/mace/modules/blocks.py:540\u001b[0m, in \u001b[0;36mRealAgnosticInteractionBlock.forward\u001b[0;34m(self, node_attrs, node_feats, edge_attrs, edge_feats, edge_index)\u001b[0m\n\u001b[1;32m    538\u001b[0m node_feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_up(node_feats)\n\u001b[1;32m    539\u001b[0m tp_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_tp_weights(edge_feats)\n\u001b[0;32m--> 540\u001b[0m mji \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_tp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode_feats\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtp_weights\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [n_edges, irreps]\u001b[39;00m\n\u001b[1;32m    543\u001b[0m message \u001b[38;5;241m=\u001b[39m scatter_sum(\n\u001b[1;32m    544\u001b[0m     src\u001b[38;5;241m=\u001b[39mmji, index\u001b[38;5;241m=\u001b[39mreceiver, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dim_size\u001b[38;5;241m=\u001b[39mnum_nodes\n\u001b[1;32m    545\u001b[0m )  \u001b[38;5;66;03m# [n_nodes, irreps]\u001b[39;00m\n\u001b[1;32m    546\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(message) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavg_num_neighbors\n",
      "File \u001b[0;32m~/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/matsciml/lib/python3.9/site-packages/e3nn/o3/_tensor_product/_tensor_product.py:529\u001b[0m, in \u001b[0;36mTensorProduct.forward\u001b[0;34m(self, x, y, weight)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;66;03m# - PROFILER - with torch.autograd.profiler.record_function(self._profiling_str):\u001b[39;00m\n\u001b[1;32m    528\u001b[0m real_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_weights(weight)\n\u001b[0;32m--> 529\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compiled_main_left_right\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/matsciml/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"<eval_with_key>.21\", line 49, in forward\n    einsum_3 = torch.functional.einsum('edb,eca->ecdab', reshape_5, reshape_3)\n    reshape_12 = reshape_3.reshape(getitem_4, 64)\n    einsum_4 = torch.functional.einsum('ca,cab->cab', reshape_12, reshape_11);  reshape_12 = reshape_11 = None\n               ~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    einsum_5 = torch.functional.einsum('dbc,dca->dba', einsum_4, reshape_5);  einsum_4 = reshape_5 = None\n    reshape_13 = einsum_5.reshape(getitem_4, 192);  einsum_5 = None\nRuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.75 GiB of which 15.12 MiB is free. Process 392269 has 174.00 MiB memory in use. Process 2007541 has 10.59 GiB memory in use. Process 1709378 has 150.00 MiB memory in use. Process 3487595 has 554.00 MiB memory in use. Including non-PyTorch memory, this process has 222.00 MiB memory in use. Of the allocated memory 82.87 MiB is allocated by PyTorch, and 17.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "counter=0\n",
    "Predictions_e=[]\n",
    "Actuals_e=[]\n",
    "for batch in train_loader:\n",
    "    batch2cuda(batch)\n",
    "    Result=task.forward(batch)\n",
    "\n",
    "    Pred_Energy=Result['regression0']['corrected_total_energy'].item()*norm_factors['energy_std']+norm_factors['energy_mean']\n",
    "    Pred_Forces=Result['gff_regression0']['force']\n",
    "\n",
    "    Actual_Energy=batch['targets']['corrected_total_energy'].item()\n",
    "    Actual_Forces=batch['targets']['force']\n",
    "\n",
    "    Predictions_e+=[Pred_Energy]\n",
    "    Actuals_e+=[Actual_Energy]\n",
    "    counter+=1\n",
    "    if(counter>10):\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Predicted')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl0AAAG0CAYAAAAIIZL8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA6L0lEQVR4nO3deXhU5f3//9cEyIQlGQIkJEhkE8EIlQIFh8oiAqEgNv1YyyJrgxaKCIIICJWgBRREpS5QKotWVEhRqhAiMYDWEg1bwIChLkGoJGAFMgEhZDm/P/hmfoxZyDI5k8k8H9d1Lphz7rnP/b5Ohnlxzsl9LIZhGAIAAEC18vP0AAAAAHwBoQsAAMAEhC4AAAATELoAAABMQOgCAAAwAaELAADABIQuAAAAExC6AAAATEDoAgAAMAGhCwAAwAQ+G7pefvlltW7dWgEBAerZs6dSUlI8PSQAAFCLWXzx2YsbN27U2LFjtWrVKvXs2VMvvPCC4uLidOzYMYWGhpb53sLCQp06dUqBgYGyWCwmjRgAAFSFYRjKyclRixYt5OfnmXNOPhm6evbsqV/84hd66aWXJF0NUhEREZo6darmzJlT5nv/+9//KiIiwoxhAgAANzt58qRatmzpkX3X9chePejKlSvav3+/5s6d61zn5+enAQMGKDk5uVj73Nxc5ebmOl8XZdSTJ08qKCio+gcMAACqzOFwKCIiQoGBgR4bg8+Frv/9738qKChQ8+bNXdY3b95c6enpxdovWbJECxcuLLY+KCiI0AUAgJfx5K1BPnsjfXnNnTtX2dnZzuXkyZOeHhIAAPBCPnemq1mzZqpTp45Onz7tsv706dMKCwsr1t5qtcpqtZo1PAAAUEv53Jkuf39/devWTUlJSc51hYWFSkpKkt1u9+DIAABAbeZzZ7okacaMGRo3bpy6d++uHj166IUXXtDFixc1YcIETw8NAADUUj4ZuoYPH67vv/9eTzzxhLKystSlSxclJCQUu7keAADAXXxynq6qcDgcstlsys7O5rcXAQDwEjXh+9vn7ukCAADwBEIXAACACQhdAAAAJvDJG+kB4KcKCg2lZJzVmZzLCg0MUI82TVTHj4faA3AfQhcAn5eQlqnY944qy3HZuS4sKECx90RqcKdwD44MQG3C5UUAPi0hLVOT3jjgErgkKctxWZPeOKCEtEwPjQxAbUPoAuCzCgoNzXnn8zLbzH3ncxUUMrMOgKojdAHwWZ9+84PO/5hXZptzP+bp029+MGlEAGozQhcAn5X8dfnCVHnbAUBZCF0AfFh5LxtyeRFA1RG6APgse9tmbm0HAGUhdAHwWb9o00TXm4nL8v/aAUBVEboA+Kz935677oVD4/+1A4CqInQB8Flnci5fv1EF2gFAWQhdAHxWaGCAW9sBQFkIXQB8Vo82TRRuCyj1vi6LpHDb1ecwAkBVEboA+Kw6fhYtGBYpScWCV9HrBcMiefA1ALcgdAHwaYM7hWvl6K5qHmR1Wd88yKqVo7vywGsAbkPoAgBJpZ/rAgD3IHQB8GkJaZma/MYBZTlcf0PxtOOyJr9xQAlpmR4aGYDahtAFwGcVFBpa+P7REufqKlq38P2jKijkMUAAqo7QBcBnpWScVWZ26XNwGZIysy8rJeOseYMCUGsRugD4LCZHBWAmQhcAn8XkqADMROgC4LOYHBWAmQhdAHwWk6MCMBOhC4BPY3JUAGYhdAGAJCZHBVDdCF0AfBqTowIwC6ELgM9iclQAZiJ0AfBZTI4KwEyELgA+i8lRAZiJ0AXAZzE5KgAzEboA+CwmRwVgJkIXAJ/F5KgAzEToAuDTiiZHDbO5XkIMswUwOSoAt6rr6QEAgKcN7hSugZFhSsk4qzM5lxUaePWSIme4ALgToQsAdPVSo71dU08PA0AtxuVFAAAAExC6AAAATEDoAgAAMIHXhK5FixapV69eatCggRo3blximxMnTmjo0KFq0KCBQkNDNWvWLOXn57u02b17t7p27Sqr1aqbbrpJ69evr/7BA6jxCgoNJX/9g/6Z+p2Sv/6B5y0CcDuvuZH+ypUruu+++2S327VmzZpi2wsKCjR06FCFhYVpz549yszM1NixY1WvXj0tXrxYkpSRkaGhQ4dq0qRJ2rBhg5KSkjRx4kSFh4crKirK7JIA1BAJaZla+P5Rl+cwhtsCtGBYJFNGAHAbi2EYXvXfufXr12v69Ok6f/68y/rt27fr7rvv1qlTp9S8eXNJ0qpVqzR79mx9//338vf31+zZs7Vt2zalpaU53zdixAidP39eCQkJ5dq/w+GQzWZTdna2goKC3FYXAM9ISMvU5DcO6Kf/EBZNFsFcXUDtUBO+v73m8uL1JCcnq3Pnzs7AJUlRUVFyOBw6cuSIs82AAQNc3hcVFaXk5ORS+83NzZXD4XBZANQOBYWGFr5/tFjgkuRct/D9o1xqBOAWtSZ0ZWVluQQuSc7XWVlZZbZxOBy6dOlSif0uWbJENpvNuURERFTD6AF4QkrGWZdLij9lSMrMvqyUjLPmDQpAreXR0DVnzhxZLJYyl/T0dE8OUXPnzlV2drZzOXnypEfHA8B9zuSUHrgq0w4AyuLRG+lnzpyp8ePHl9mmbdu25eorLCxMKSkpLutOnz7t3Fb0Z9G6a9sEBQWpfv36JfZrtVpltVrLNQYA3iU0MOD6jSrQDgDK4tHQFRISopCQELf0ZbfbtWjRIp05c0ahoaGSpMTERAUFBSkyMtLZJj4+3uV9iYmJstvtbhkDAO/So00ThdsClJV9ucT7uiy6+uDrHm2amD00ALWQ19zTdeLECaWmpurEiRMqKChQamqqUlNTdeHCBUnSoEGDFBkZqTFjxujQoUP64IMPNH/+fE2ZMsV5pmrSpEn65ptv9Nhjjyk9PV2vvPKKNm3apEceecSTpQHwkDp+Fi0YdvU/ZT99tHXR6wXDInnwNQC38JopI8aPH6/XXnut2Ppdu3apX79+kqRvv/1WkydP1u7du9WwYUONGzdOTz/9tOrW/f9P6O3evVuPPPKIjh49qpYtW+pPf/rTdS9xXqsm/MopAPdini6g9qsJ399eE7pqippw0AC4X0GhoZSMszqTc1mhgVcvKXKGC6g9asL3t9fMSA8A1amOn0X2dk09PQwAtZjX3NMFAADgzQhdAAAAJiB0AQAAmIDQBQAAYAJCFwAAgAkIXQAAACYgdAEAAJiAeboAQEyOCqD6EboA+DweAwTADFxeBODTEtIyNfmNAy6BS5Kysi9r8hsHlJCW6aGRAahtCF0AfFZBoaGF7x9VSQ+gLVq38P2jKijkEbUAqo7QBcBnpWScLXaG61qGpMzsy0rJOGveoADUWoQuAD7rTE7pgasy7QCgLIQuAD4rNDDAre0AoCyELgA+q1urYF1vVgg/y9V2AFBVhC4APmv/t+d0vXvkC42r7QCgqghdAHwW93QBMBOhC4DP4p4uAGYidAHwWT3aNFG4LUCl3dZl0dWZ6Xu0aWLmsADUUoQuAD6rjp9FC4ZFSlKx4FX0esGwSJ7BCMAtCF0AfNrgTuFaObqrwmyulxDDbAFaOborz14E4DY88BqAzxvcKVwDI8OUknFWZ3IuKzTw6iVFznABcCdCFwDo6qVGe7umnh4GgFqMy4sAAAAmIHQBAACYgNAFAABgAkIXAACACQhdAAAAJiB0AQAAmIDQBQAAYAJCFwAAgAkIXQAAACYgdAEAAJiA0AUAAGACQhcAAIAJCF0AAAAmIHQBAACYgNAFAABgAkIXAACACbwidB0/flwxMTFq06aN6tevr3bt2mnBggW6cuWKS7vDhw+rd+/eCggIUEREhJYuXVqsr7i4OHXs2FEBAQHq3Lmz4uPjzSoDAAD4MK8IXenp6SosLNRf//pXHTlyRM8//7xWrVqlxx9/3NnG4XBo0KBBatWqlfbv369ly5YpNjZWq1evdrbZs2ePRo4cqZiYGB08eFDR0dGKjo5WWlqaJ8oCAAA+xGIYhuHpQVTGsmXLtHLlSn3zzTeSpJUrV2revHnKysqSv7+/JGnOnDnasmWL0tPTJUnDhw/XxYsXtXXrVmc/t99+u7p06aJVq1aVa78Oh0M2m03Z2dkKCgpyc1UAAKA61ITvb68401WS7OxsNWnSxPk6OTlZffr0cQYuSYqKitKxY8d07tw5Z5sBAwa49BMVFaXk5ORS95ObmyuHw+GyAAAAVJRXhq6vvvpKL774ov7whz8412VlZal58+Yu7YpeZ2VlldmmaHtJlixZIpvN5lwiIiLcVQYAAPAhHg1dc+bMkcViKXMpujRY5LvvvtPgwYN133336YEHHqj2Mc6dO1fZ2dnO5eTJk9W+TwAAUPvU9eTOZ86cqfHjx5fZpm3bts6/nzp1Snfeead69erlcoO8JIWFhen06dMu64peh4WFldmmaHtJrFarrFbrdWsBAAAoi0dDV0hIiEJCQsrV9rvvvtOdd96pbt26ad26dfLzcz1JZ7fbNW/ePOXl5alevXqSpMTERHXo0EHBwcHONklJSZo+fbrzfYmJibLb7e4pCAAAoBRecU/Xd999p379+unGG2/Us88+q++//15ZWVku92KNGjVK/v7+iomJ0ZEjR7Rx40atWLFCM2bMcLaZNm2aEhIStHz5cqWnpys2Nlb79u3TQw895ImyAACAD/Homa7ySkxM1FdffaWvvvpKLVu2dNlWNOOFzWbTjh07NGXKFHXr1k3NmjXTE088oQcffNDZtlevXnrzzTc1f/58Pf7442rfvr22bNmiTp06mVoPAADwPV47T5en1IR5PgAAQMXUhO9vr7i8CAAA4O0IXQAAACYgdAEAAJiA0AUAAGACQhcAAIAJCF0AAAAmIHQBAACYgNAFAABgAkIXAACACQhdAAAAJiB0AQAAmIDQBQAAYAJCFwAAgAkIXQAAACYgdAEAAJiA0AUAAGACQhcAAIAJCF0AAAAmIHQBAACYgNAFAABgAkIXAACACQhdAAAAJiB0AQAAmIDQBQAAYAJCFwAAgAkIXQAAACYgdAEAAJiA0AUAAGACQhcAAIAJCF0AAAAmIHQBAACYoK6nBwAANUFBoaGUjLM6k3NZoYEB6tGmier4WTw9LAC1CKELgM9LSMvUwvePKjP7snNduC1AC4ZFanCncA+ODEBtwuVFAD4tIS1Tk9844BK4JCkr+7Imv3FACWmZHhoZgNqG0AXAZxUUGlr4/lEZJWwrWrfw/aMqKCypBQBUDKELgM9KyThb7AzXtQxJmdmXlZJx1rxBAai1CF0AfNaZnNIDV2XaAUBZCF0AfFZoYIBb2wFAWQhdAHxWjzZNFG4LUGkTQ1h09bcYe7RpYuawANRShC4APquOn0ULhkVKUrHgVfR6wbBI5usC4BaELgA+bXCncK0c3VVhNtdLiGG2AK0c3ZV5ugC4jdeErnvuuUc33nijAgICFB4erjFjxujUqVMubQ4fPqzevXsrICBAERERWrp0abF+4uLi1LFjRwUEBKhz586Kj483qwQANdTgTuH6ZHZ/vfXA7VoxooveeuB2fTK7P4ELgFt5Tei68847tWnTJh07dkybN2/W119/rd/+9rfO7Q6HQ4MGDVKrVq20f/9+LVu2TLGxsVq9erWzzZ49ezRy5EjFxMTo4MGDio6OVnR0tNLS0jxREoAapI6fRfZ2TfXrLjfI3q4plxQBuJ3FMAyvnPXvvffeU3R0tHJzc1WvXj2tXLlS8+bNU1ZWlvz9/SVJc+bM0ZYtW5Seni5JGj58uC5evKitW7c6+7n99tvVpUsXrVq1qlz7dTgcstlsys7OVlBQkPsLAwAAblcTvr/LfabL4XCUe6luZ8+e1YYNG9SrVy/Vq1dPkpScnKw+ffo4A5ckRUVF6dixYzp37pyzzYABA1z6ioqKUnJycqn7ys3NNb0+AABQ+5Q7dDVu3FjBwcHlWqrL7Nmz1bBhQzVt2lQnTpzQP//5T+e2rKwsNW/e3KV90eusrKwy2xRtL8mSJUtks9mcS0REhLvKAQAAPqTcoWvXrl3auXOndu7cqbVr1yo0NFSPPfaY3n33Xb377rt67LHH1Lx5c61du7bcO58zZ44sFkuZS9GlQUmaNWuWDh48qB07dqhOnToaO3asqvvq6Ny5c5Wdne1cTp48Wa37AwAAtVPd8jbs27ev8+9PPvmknnvuOY0cOdK57p577lHnzp21evVqjRs3rlx9zpw5U+PHjy+zTdu2bZ1/b9asmZo1a6abb75Zt9xyiyIiIvTpp5/KbrcrLCxMp0+fdnlv0euwsDDnnyW1KdpeEqvVKqvVWq56AAAASlPu0HWt5OTkEm887969uyZOnFjufkJCQhQSElKZIaiwsFDS1XuuJMlut2vevHnKy8tz3ueVmJioDh06OC952u12JSUlafr06c5+EhMTZbfbKzUGAACA8qrUlBERERH629/+Vmz9q6++Wi33PH322Wd66aWXlJqaqm+//VY7d+7UyJEj1a5dO2dgGjVqlPz9/RUTE6MjR45o48aNWrFihWbMmOHsZ9q0aUpISNDy5cuVnp6u2NhY7du3Tw899JDbxwwAAHCtSk0ZER8fr3vvvVc33XSTevbsKUlKSUnRl19+qc2bN2vIkCFuHeTnn3+uadOm6dChQ7p48aLCw8M1ePBgzZ8/XzfccIOz3eHDhzVlyhTt3btXzZo109SpUzV79myXvuLi4jR//nwdP35c7du319KlSys03prwK6cAAKBiasL3d6Xn6Tp58qRWrlzpvNH9lltu0aRJk2r9b/fVhIMGAAAqpiZ8f3vt5KieUhMOGgAAqJia8P1d6ccA/etf/9Lo0aPVq1cvfffdd5Kkv//97/rkk0/cNjgAAIDaolKha/PmzYqKilL9+vV14MAB528QZmdna/HixW4dIAAAQG1QqdD15z//WatWrdLf/vY35/QMkvTLX/5SBw4ccNvgAAAAaotKha5jx46pT58+xdbbbDadP3++qmMCAACodSoVusLCwvTVV18VW//JJ5+4zCAPAACAqyoVuh544AFNmzZNn332mSwWi06dOqUNGzbo0Ucf1eTJk909RgAAAK9XqccAzZkzR4WFhbrrrrv0448/qk+fPrJarXr00Uc1depUd48RAADA61Vpnq4rV67oq6++0oULFxQZGalGjRq5c2w1Uk2Y5wMAAFRMTfj+rtTlxd///vfKycmRv7+/IiMj1aNHDzVq1EgXL17U73//e3ePEQAAwOtVKnS99tprunTpUrH1ly5d0uuvv17lQQEAANQ2Fbqny+FwyDAMGYahnJwcBQQEOLcVFBQoPj5eoaGhbh8kAACAt6tQ6GrcuLEsFossFotuvvnmYtstFosWLlzotsEBAADUFhUKXbt27ZJhGOrfv782b96sJk2aOLf5+/urVatWatGihdsHCQAA4O0qFLr69u0rScrIyNCNN94oi8VSLYMCAACobSp1I/3OnTv1j3/8o9j6uLg4vfbaa1UeFAAAQG1TqdC1ZMkSNWvWrNj60NBQLV68uMqDAgAAqG0qFbpOnDihNm3aFFvfqlUrnThxosqDAgAAqG0qFbpCQ0N1+PDhYusPHTqkpk2bVnlQAAAAtU2lQtfIkSP18MMPa9euXSooKFBBQYF27typadOmacSIEe4eIwAAgNer1AOvn3rqKR0/flx33XWX6ta92kVhYaHGjh3LPV0AAAAlqNIDr//zn//o0KFDql+/vjp37qxWrVq5c2w1Uk14YCYAAKiYmvD9XakzXUVuvvnmEmemBwAAgKtyh64ZM2boqaeeUsOGDTVjxowy2z733HNVHhgAAEBtUu7QdfDgQeXl5Tn/XhpmqQcAACiuSvd0+aKacE0YAABUTE34/q7UlBEAAAComHJfXvy///u/cnf6zjvvVGowAAAAtVW5z3TZbDbnEhQUpKSkJO3bt8+5ff/+/UpKSpLNZquWgQIAAHizcp/pWrdunfPvs2fP1u9+9zutWrVKderUkSQVFBToj3/8I/c5AQAAlKBSN9KHhITok08+UYcOHVzWHzt2TL169dIPP/zgtgHWNDXhRjwAAFAxNeH7u1I30ufn5ys9Pb3Y+vT0dBUWFlZ5UAAAALVNpWaknzBhgmJiYvT111+rR48ekqTPPvtMTz/9tCZMmODWAQIAANQGlQpdzz77rMLCwrR8+XJlZmZKksLDwzVr1izNnDnTrQMEAACoDao8OarD4ZAkn7m/qSZcEwYAABVTE76/Kz05an5+vj788EO99dZbzkf/nDp1ShcuXHDb4AAAAGqLSl1e/PbbbzV48GCdOHFCubm5GjhwoAIDA/XMM88oNzdXq1atcvc4AQAAvFqlznRNmzZN3bt317lz51S/fn3n+t/85jdKSkpy2+AAwCwFhYaSv/5B/0z9Tslf/6CCQh5LC8C9KnWm61//+pf27Nkjf39/l/WtW7fWd99955aBAYBZEtIytfD9o8rMvuxcF24L0IJhkRrcKdyDIwNQm1TqTFdhYaEKCgqKrf/vf/+rwMDAKg8KAMySkJapyW8ccAlckpSVfVmT3zighLRMD40MQG1TqdA1aNAgvfDCC87XFotFFy5c0IIFCzRkyBB3ja1Eubm56tKliywWi1JTU122HT58WL1791ZAQIAiIiK0dOnSYu+Pi4tTx44dFRAQoM6dOys+Pr5axwug5iooNLTw/aMq6UJi0bqF7x/lUiMAt6hU6Hr22Wf173//W5GRkbp8+bJGjRrlvLT4zDPPuHuMLh577DG1aNGi2HqHw6FBgwapVatW2r9/v5YtW6bY2FitXr3a2WbPnj0aOXKkYmJidPDgQUVHRys6OlppaWnVOmYANVNKxtliZ7iuZUjKzL6slIyz5g0KQK1VqXu6IiIidOjQIW3cuFGHDh3ShQsXFBMTo/vvv9/lxnp32759u3bs2KHNmzdr+/btLts2bNigK1euaO3atfL399ett96q1NRUPffcc3rwwQclSStWrNDgwYM1a9YsSdJTTz2lxMREvfTSS/zGJeCDzuSUHrgq0w4AylLh0JWXl6eOHTtq69atuv/++3X//fdXx7iKOX36tB544AFt2bJFDRo0KLY9OTlZffr0cbm5PyoqSs8884zOnTun4OBgJScna8aMGS7vi4qK0pYtW0rdb25urnJzc52viyaDBeD9QgMD3NoOAMpS4cuL9erV0+XL5v6vzzAMjR8/XpMmTVL37t1LbJOVlaXmzZu7rCt6nZWVVWabou0lWbJkiWw2m3OJiIioSikAapAebZoo3BYgSynbLbr6W4w92jQxc1gAaqlK3dM1ZcoUPfPMM8rPz6/SzufMmSOLxVLmkp6erhdffFE5OTmaO3dulfZXGXPnzlV2drZzOXnypOljAFA96vhZtGBYpCQVC15FrxcMi1Qdv9JiGQCUX6Xu6dq7d6+SkpK0Y8cOde7cWQ0bNnTZ/s4775Srn5kzZ2r8+PFltmnbtq127typ5ORkWa1Wl23du3fX/fffr9dee01hYWE6ffq0y/ai12FhYc4/S2pTtL0kVqu12H4B1B6DO4Vr5eiuxebpCmOeLgBuVqnQ1bhxY917771V3nlISIhCQkKu2+4vf/mL/vznPztfnzp1SlFRUdq4caN69uwpSbLb7Zo3b57y8vJUr149SVJiYqI6dOig4OBgZ5ukpCRNnz7d2VdiYqLsdnuVawHgvQZ3CtfAyDClZJzVmZzLCg28ekmRM1wA3KlCoauwsFDLli3Tf/7zH125ckX9+/dXbGxstf7GoiTdeOONLq8bNWokSWrXrp1atmwpSRo1apQWLlyomJgYzZ49W2lpaVqxYoWef/555/umTZumvn37avny5Ro6dKjefvtt7du3z2VaCQC+qY6fRfZ2TT09DAC1WIXu6Vq0aJEef/xxNWrUSDfccIP+8pe/aMqUKdU1tgqx2WzasWOHMjIy1K1bN82cOVNPPPGEc7oISerVq5fefPNNrV69Wrfddpv+8Y9/aMuWLerUqZMHRw4AAHyBxTCMck+13L59ez366KP6wx/+IEn68MMPNXToUF26dEl+fpW6J9/rOBwO2Ww2ZWdnKygoyNPDAQAA5VATvr8rlJROnDjh8pifAQMGyGKx6NSpU24fGAAAQG1SodCVn5+vgADXSQLr1aunvLw8tw4KAACgtqnQjfRFk5ReO4XC5cuXNWnSJJdpI8o7ZQQAAICvqFDoGjduXLF1o0ePdttgAAAAaqsKha5169ZV1zgAAABqNd/4lUMAAAAPI3QBAACYgNAFAABgAkIXAACACQhdAAAAJiB0AQAAmIDQBQAAYAJCFwAAgAkIXQAAACYgdAEAAJiA0AUAAGACQhcAAIAJCF0AAAAmIHQBAACYgNAFAABgAkIXAACACQhdAAAAJiB0AQAAmIDQBQAAYAJCFwAAgAkIXQAAACYgdAEAAJiA0AUAAGACQhcAAIAJCF0AAAAmIHQBAACYgNAFAABgAkIXAACACQhdAAAAJiB0AQAAmIDQBQAAYAJCFwAAgAkIXQAAACYgdAEAAJiA0AUAAGACQhcAAIAJvCZ0tW7dWhaLxWV5+umnXdocPnxYvXv3VkBAgCIiIrR06dJi/cTFxaljx44KCAhQ586dFR8fb1YJAADAh3lN6JKkJ598UpmZmc5l6tSpzm0Oh0ODBg1Sq1attH//fi1btkyxsbFavXq1s82ePXs0cuRIxcTE6ODBg4qOjlZ0dLTS0tI8UQ4AAPAhdT09gIoIDAxUWFhYids2bNigK1euaO3atfL399ett96q1NRUPffcc3rwwQclSStWrNDgwYM1a9YsSdJTTz2lxMREvfTSS1q1apVpdQAAAN/jVWe6nn76aTVt2lQ///nPtWzZMuXn5zu3JScnq0+fPvL393eui4qK0rFjx3Tu3DlnmwEDBrj0GRUVpeTk5FL3mZubK4fD4bIAAABUlNec6Xr44YfVtWtXNWnSRHv27NHcuXOVmZmp5557TpKUlZWlNm3auLynefPmzm3BwcHKyspyrru2TVZWVqn7XbJkiRYuXOjmagAAgK/x6JmuOXPmFLs5/qdLenq6JGnGjBnq16+ffvazn2nSpElavny5XnzxReXm5lbrGOfOnavs7GzncvLkyWrdHwAAqJ08eqZr5syZGj9+fJlt2rZtW+L6nj17Kj8/X8ePH1eHDh0UFham06dPu7Qpel10H1hpbUq7T0ySrFarrFbr9UoBAAAok0dDV0hIiEJCQir13tTUVPn5+Sk0NFSSZLfbNW/ePOXl5alevXqSpMTERHXo0EHBwcHONklJSZo+fbqzn8TERNnt9qoVAgAAcB1ecSN9cnKyXnjhBR06dEjffPONNmzYoEceeUSjR492BqpRo0bJ399fMTExOnLkiDZu3KgVK1ZoxowZzn6mTZumhIQELV++XOnp6YqNjdW+ffv00EMPeao0AADgIyyGYRieHsT1HDhwQH/84x+Vnp6u3NxctWnTRmPGjNGMGTNcLv0dPnxYU6ZM0d69e9WsWTNNnTpVs2fPdukrLi5O8+fP1/Hjx9W+fXstXbpUQ4YMKfdYHA6HbDabsrOzFRQU5LYaAQBA9akJ399eEbpqkppw0AAAQMXUhO9vr7i8CAAA4O0IXQAAACYgdAEAAJiA0AUAAGACQhcAAIAJCF0AAAAmIHQBAACYgNAFAABgAkIXAACACQhdAAAAJiB0AQAAmIDQBQAAYAJCFwAAgAkIXQAAACYgdAEAAJiA0AUAAGACQhcAAIAJCF0AAAAmIHQBAACYgNAFAABgAkIXAACACQhdAAAAJiB0AQAAmIDQBQAAYAJCFwAAgAkIXQAAACYgdAEAAJiA0AUAAGACQhcAAIAJCF0AAAAmIHQBAACYgNAFAABgAkIXAACACQhdAAAAJiB0AQAAmIDQBQAAYAJCFwAAgAkIXQAAACYgdAEAAJiA0AUAAGACrwpd27ZtU8+ePVW/fn0FBwcrOjraZfuJEyc0dOhQNWjQQKGhoZo1a5by8/Nd2uzevVtdu3aV1WrVTTfdpPXr15tXAAAA8Fl1PT2A8tq8ebMeeOABLV68WP3791d+fr7S0tKc2wsKCjR06FCFhYVpz549yszM1NixY1WvXj0tXrxYkpSRkaGhQ4dq0qRJ2rBhg5KSkjRx4kSFh4crKirKU6UBAAAfYDEMw/D0IK4nPz9frVu31sKFCxUTE1Nim+3bt+vuu+/WqVOn1Lx5c0nSqlWrNHv2bH3//ffy9/fX7NmztW3bNpewNmLECJ0/f14JCQnlGovD4ZDNZlN2draCgoKqXhwAAKh2NeH72ysuLx44cEDfffed/Pz89POf/1zh4eH61a9+5RKekpOT1blzZ2fgkqSoqCg5HA4dOXLE2WbAgAEufUdFRSk5ObnUfefm5srhcLgsAAAAFeUVoeubb76RJMXGxmr+/PnaunWrgoOD1a9fP509e1aSlJWV5RK4JDlfZ2VlldnG4XDo0qVLJe57yZIlstlsziUiIsKttQEAAN/g0dA1Z84cWSyWMpf09HQVFhZKkubNm6d7771X3bp107p162SxWBQXF1etY5w7d66ys7Ody8mTJ6t1fwAAoHby6I30M2fO1Pjx48ts07ZtW2VmZkqSIiMjneutVqvatm2rEydOSJLCwsKUkpLi8t7Tp087txX9WbTu2jZBQUGqX79+ifu3Wq2yWq3lLwoAAKAEHg1dISEhCgkJuW67bt26yWq16tixY7rjjjskSXl5eTp+/LhatWolSbLb7Vq0aJHOnDmj0NBQSVJiYqKCgoKcYc1utys+Pt6l78TERNntdneWBQAAUIxX3NMVFBSkSZMmacGCBdqxY4eOHTumyZMnS5Luu+8+SdKgQYMUGRmpMWPG6NChQ/rggw80f/58TZkyxXmmatKkSfrmm2/02GOPKT09Xa+88oo2bdqkRx55xGO1AQAA3+A183QtW7ZMdevW1ZgxY3Tp0iX17NlTO3fuVHBwsCSpTp062rp1qyZPniy73a6GDRtq3LhxevLJJ519tGnTRtu2bdMjjzyiFStWqGXLlnr11VeZowsAAFQ7r5inqyapCfN8AACAiqkJ399ecXkRAADA2xG6AAAATEDoAgAAMAGhCwAAwASELgAAABMQugAAAExA6AIAADABoQsAAMAEhC4AAAATELoAAABMQOgCAAAwAaELAADABIQuAAAAExC6AAAATEDoAgAAMAGhCwAAwASELgAAABMQugAAAExA6AIAADABoQsAAMAEhC4AAAATELoAAABMQOgCAAAwAaELAADABIQuAAAAExC6AAAATEDoAgAAMAGhCwAAwASELgAAABMQugAAAExQ19MDAICaoKDQUErGWZ3JuazQwAD1aNNEdfwsnh4WgFqE0AXA5yWkZWrh+0eVmX3ZuS7cFqAFwyI1uFO4B0cGoDbh8iIAn5aQlqnJbxxwCVySlJV9WZPfOKCEtEwPjQxAbUPoAuCzCgoNLXz/qIwSthWtW/j+URUUltQCACqG0AXAZ6VknC12hutahqTM7MtKyThr3qAA1FqELgA+60xO6YGrMu0AoCyELgA+KzQwwK3tAKAshC4APqtHmyYKtwWotIkhLLr6W4w92jQxc1gAailCFwCfVcfPogXDIiWpWPAqer1gWCTzdQFwC0IXAJ82uFO4Vo7uqjCb6yXEMFuAVo7uyjxdANyGyVEB+LzBncI1MDKMGekBVCuvONO1e/duWSyWEpe9e/c62x0+fFi9e/dWQECAIiIitHTp0mJ9xcXFqWPHjgoICFDnzp0VHx9vZikAaqg6fhbZ2zXVr7vcIHu7pgQuAG7nFaGrV69eyszMdFkmTpyoNm3aqHv37pIkh8OhQYMGqVWrVtq/f7+WLVum2NhYrV692tnPnj17NHLkSMXExOjgwYOKjo5WdHS00tLSPFUaAADwERbDMLxuquW8vDzdcMMNmjp1qv70pz9JklauXKl58+YpKytL/v7+kqQ5c+Zoy5YtSk9PlyQNHz5cFy9e1NatW5193X777erSpYtWrVpVrn07HA7ZbDZlZ2crKCjIzZUBAIDqUBO+v73iTNdPvffee/rhhx80YcIE57rk5GT16dPHGbgkKSoqSseOHdO5c+ecbQYMGODSV1RUlJKTk0vdV25urhwOh8sCAABQUV4ZutasWaOoqCi1bNnSuS4rK0vNmzd3aVf0Oisrq8w2RdtLsmTJEtlsNucSERHhrjIAAIAP8WjomjNnTqk3yBctRZcGi/z3v//VBx98oJiYGFPGOHfuXGVnZzuXkydPmrJfAABQu3h0yoiZM2dq/PjxZbZp27aty+t169apadOmuueee1zWh4WF6fTp0y7ril6HhYWV2aZoe0msVqusVmuZYwQAALgej4aukJAQhYSElLu9YRhat26dxo4dq3r16rlss9vtmjdvnvLy8pzbEhMT1aFDBwUHBzvbJCUlafr06c73JSYmym63V70YAACAMnjVPV07d+5URkaGJk6cWGzbqFGj5O/vr5iYGB05ckQbN27UihUrNGPGDGebadOmKSEhQcuXL1d6erpiY2O1b98+PfTQQ2aWAQAAfJBXha41a9aoV69e6tixY7FtNptNO3bsUEZGhrp166aZM2fqiSee0IMPPuhs06tXL7355ptavXq1brvtNv3jH//Qli1b1KlTJzPLAAAAPsgr5+nypOzsbDVu3FgnT55kni4AALyEw+FQRESEzp8/L5vN5pEx8OzFCsrJyZEkpo4AAMAL5eTkeCx0caarggoLC3Xq1CkFBgbKYqk9z2Yr+h+AL57Bo3Zq96XafbVuidp9sfZr6w4MDFROTo5atGghPz/P3F3Fma4K8vPzc5mUtbYJCgryqQ/ktaid2n2Jr9YtUbsv1l5Ut6fOcBXxqhvpAQAAvBWhCwAAwASELki6OvP+ggULfHL2fWqndl/iq3VL1O6Ltde0urmRHgAAwASc6QIAADABoQsAAMAEhC4AAAATELoAAABMQOiqpRYtWqRevXqpQYMGaty4cYlt9u7dq7vuukuNGzdWcHCwoqKidOjQIef22NhYWSyWYkvDhg3L3HdJ73n77bfdWV6Z3FH78ePHS6zj008/LXPfJ06c0NChQ9WgQQOFhoZq1qxZys/Pd2d5ZXJH7bt379avf/1rhYeHq2HDhurSpYs2bNhw3X178ri7o25JOnz4sHr37q2AgABFRERo6dKl1913TT/m69evL/HYWCwWnTlzRpI0fvz4Erffeuutpe63sp8Rd3JH7bt37y5xe1ZWVpn7rszPiju5o/Z33nlHAwcOVEhIiIKCgmS32/XBBx+UuV9PH3d31C1dPe5du3aV1WrVTTfdpPXr11933+445oSuWurKlSu67777NHny5BK3X7hwQYMHD9aNN96ozz77TJ988okCAwMVFRWlvLw8SdKjjz6qzMxMlyUyMlL33Xffdfe/bt06l/dFR0e7s7wyuaP2Ih9++KFLHd26dSt1vwUFBRo6dKiuXLmiPXv26LXXXtP69ev1xBNPuLW+srij9j179uhnP/uZNm/erMOHD2vChAkaO3astm7det39e+q4u6Nuh8OhQYMGqVWrVtq/f7+WLVum2NhYrV69utT9esMxHz58eLHPcVRUlPr27avQ0FBJ0ooVK1y2nzx5Uk2aNCnXZ70inxF3c0ftRY4dO+bS7qfbr1WZnxV3c0ftH3/8sQYOHKj4+Hjt379fd955p4YNG6aDBw9ed/+eOu7uqDsjI0NDhw7VnXfeqdTUVE2fPl0TJ04sM3C67ZgbqNXWrVtn2Gy2Yuv37t1rSDJOnDjhXHf48GFDkvHll1+W2Fdqaqohyfj444/L3Kck4913363KsN2iKrVnZGQYkoyDBw+We3/x8fGGn5+fkZWV5Vy3cuVKIygoyMjNza10HZXhzuNuGIYxZMgQY8KECWXusyYc96rU/corrxjBwcEux2r27NlGhw4dSt2fNxzznzpz5oxRr1494/XXXy+1zbvvvmtYLBbj+PHjpbapzGekulSl9l27dhmSjHPnzpV7f5X5Waku7jzuhmEYkZGRxsKFC0vdXlOOe1Xqfuyxx4xbb73Vpd3w4cONqKioUvtx1zHnTJeP6tChg5o2bao1a9boypUrunTpktasWaNbbrlFrVu3LvE9r776qm6++Wb17t37uv1PmTJFzZo1U48ePbR27VoZNWg6uIrUfs899yg0NFR33HGH3nvvvTL7TU5OVufOndW8eXPnuqioKDkcDh05cqQ6Sqmwyhx3ScrOzlaTJk2u239NPe7lqTs5OVl9+vSRv7+/831RUVE6duyYzp07V2K/3nDMf+r1119XgwYN9Nvf/rbUNmvWrNGAAQPUqlWr6/ZXkc+Ip5VVe5cuXRQeHq6BAwfq3//+d5n9VOZnxdPKc9wLCwuVk5NTrs+6txz3kupOTk7WgAEDXNpFRUUpOTm51H7cdcwJXT4qMDBQu3fv1htvvKH69eurUaNGSkhI0Pbt21W3bvHnoF++fFkbNmxQTEzMdft+8skntWnTJiUmJuree+/VH//4R7344ovVUUallKf2Ro0aafny5YqLi9O2bdt0xx13KDo6usx/XLKysly+fCU5X1/v/hCzVPS4S9KmTZu0d+9eTZgwocy+a/JxL0/dlTl+3nDMf2rNmjUaNWqU6tevX+L2U6dOafv27Zo4cWKZ/VTmM+JpJdUeHh6uVatWafPmzdq8ebMiIiLUr18/HThwoNR+auNxl6Rnn31WFy5c0O9+97tS23jbcS+p7tKOn8Ph0KVLl0rsx23HvELnxeBRs2fPNiSVuXzxxRcu7yntFOyPP/5o9OjRwxg7dqyRkpJiJCcnG/fee69x6623Gj/++GOx9m+++aZRt25dl8so5fWnP/3JaNmyZYXfdy1P1l5kzJgxxh133FHq9gceeMAYNGiQy7qLFy8akoz4+PiKFXwNT9a+c+dOo0GDBsZrr71W4XFX9bibXffAgQONBx980OV9R44cMSQZR48eLXGM3nDMr7Vnzx5DkrFv375S2yxevNho2rRppS6PXu8zUh6erL1Inz59jNGjR5e6vTI/K+Xhydo3bNhgNGjQwEhMTKzwuKt63M2uu3379sbixYtd1m3bts2QVOp3gLuOecn/tUWNNHPmTI0fP77MNm3bti1XX2+++aaOHz+u5ORk+fn5OdcFBwfrn//8p0aMGOHS/tVXX9Xdd99dLOmXR8+ePfXUU08pNze30s+/8mTtRXr27KnExMRS+w0LC1NKSorLutOnTzu3VZanav/oo480bNgwPf/88xo7dmyFx13V42523WFhYc7jVeR6x88bjvm1Xn31VXXp0qXUm54Nw9DatWs1ZswYl8so5XW9z0h5eKr2a/Xo0UOffPJJqdsr87NSHp6q/e2339bEiRMVFxdX7LJbeVT1uJtdd2nHLygoqNQzge465oQuLxISEqKQkBC39PXjjz/Kz89PFovFua7odWFhoUvbjIwM7dq1q9Knj1NTUxUcHFylB456qvZrpaamKjw8vNTtdrtdixYt0pkzZ5y/JZOYmKigoCBFRkZWeryeqH337t26++679cwzz+jBBx+s1L6qetzNrttut2vevHnKy8tTvXr1JF09fh06dFBwcHCJ/XrDMS9y4cIFbdq0SUuWLCm1zUcffaSvvvqqXLcRlOR6n5Hy8FTt1yrPZ72iPyvl4Yna33rrLf3+97/X22+/raFDh1ZqH1U97mbXbbfbFR8f77IuMTFRdru91P7cdszLfU4MXuXbb781Dh48aCxcuNBo1KiRcfDgQePgwYNGTk6OYRiG8cUXXxhWq9WYPHmycfToUSMtLc0YPXq0YbPZjFOnTrn0NX/+fKNFixZGfn5+sf288847Lr+98d577xl/+9vfjM8//9z48ssvjVdeecVo0KCB8cQTT1RvwddwR+3r16833nzzTeOLL74wvvjiC2PRokWGn5+fsXbt2lJrz8/PNzp16mQMGjTISE1NNRISEoyQkBBj7ty5XlV70SXFuXPnGpmZmc7lhx9+KLV2Tx93d9R9/vx5o3nz5saYMWOMtLQ04+233zYaNGhg/PWvfy21bm845kVeffVVIyAgoMzf0hs9erTRs2fPEre9+OKLRv/+/Z2vy/MZqW7uqP355583tmzZYnz55ZfG559/bkybNs3w8/MzPvzwQ2ebn9Zenp+V6uaO2jds2GDUrVvXePnll10+6+fPn3e2qWnH3R11f/PNN0aDBg2MWbNmGV988YXx8ssvG3Xq1DESEhKcbarrmBO6aqlx48aVeF18165dzjY7duwwfvnLXxo2m80IDg42+vfvbyQnJ7v0U1BQYLRs2dJ4/PHHS9zPunXrjGuz+/bt240uXboYjRo1Mho2bGjcdtttxqpVq4yCgoJqqbMk7qh9/fr1xi233GI0aNDACAoKMnr06GHExcW57OentRuGYRw/ftz41a9+ZdSvX99o1qyZMXPmTCMvL69a672WO2ovrY++ffs629S04+6un/dDhw4Zd9xxh2G1Wo0bbrjBePrpp122e+sxNwzDsNvtxqhRo0rt5/z580b9+vWN1atXl7h9wYIFRqtWrZyvy/MZqW7uqP2ZZ54x2rVrZwQEBBhNmjQx+vXrZ+zcudOlzU9rN4zr/6xUN3fU3rdv3xL7GDdunLNNTTvu7vp537Vrl9GlSxfD39/faNu2rbFu3TqX7dV1zC2GUUN+pxsAAKAWY8oIAAAAExC6AAAATEDoAgAAMAGhCwAAwASELgAAABMQugAAAExA6AIAADABoQsAqonFYtGWLVs8PQwANQShC0CtkJycrDp16lT4+XGtW7fWCy+8UD2DAoBrELoA1Apr1qzR1KlT9fHHH+vUqVOeHg4AFEPoAuD1Lly4oI0bN2ry5MkaOnSo1q9f77L9/fff1y9+8QsFBASoWbNm+s1vfiNJ6tevn7799ls98sgjslgsslgskqTY2Fh16dLFpY8XXnhBrVu3dr7eu3evBg4cqGbNmslms6lv3746cOBAdZYJwMsRugB4vU2bNqljx47q0KGDRo8erbVr16rosbLbtm3Tb37zGw0ZMkQHDx5UUlKSevToIUl655131LJlSz355JPKzMxUZmZmufeZk5OjcePG6ZNPPtGnn36q9u3ba8iQIcrJyamWGgF4v7qeHgAAVNWaNWs0evRoSdLgwYOVnZ2tjz76SP369dOiRYs0YsQILVy40Nn+tttukyQ1adJEderUUWBgoMLCwiq0z/79+7u8Xr16tRo3bqyPPvpId999dxUrAlAbcaYLgFc7duyYUlJSNHLkSElS3bp1NXz4cK1Zs0aSlJqaqrvuusvt+z19+rQeeOABtW/fXjabTUFBQbpw4YJOnDjh9n0BqB040wXAq61Zs0b5+flq0aKFc51hGLJarXrppZdUv379Cvfp5+fnvDxZJC8vz+X1uHHj9MMPP2jFihVq1aqVrFar7Ha7rly5UrlCANR6nOkC4LXy8/P1+uuva/ny5UpNTXUuhw4dUosWLfTWW2/pZz/7mZKSkkrtw9/fXwUFBS7rQkJClJWV5RK8UlNTXdr8+9//1sMPP6whQ4bo1ltvldVq1f/+9z+31gegduFMFwCvtXXrVp07d04xMTGy2Wwu2+69916tWbNGy5Yt01133aV27dppxIgRys/PV3x8vGbPni3p6jxdH3/8sUaMGCGr1apmzZqpX79++v7777V06VL99re/VUJCgrZv366goCBn/+3bt9ff//53de/eXQ6HQ7NmzarUWTUAvoMzXQC81po1azRgwIBigUu6Grr27dunJk2aKC4uTu+99566dOmi/v37KyUlxdnuySef1PHjx9WuXTuFhIRIkm655Ra98sorevnll3XbbbcpJSVFjz76aLF9nzt3Tl27dtWYMWP08MMPKzQ0tHoLBuDVLMZPb1wAAACA23GmCwAAwASELgAAABMQugAAAExA6AIAADABoQsAAMAEhC4AAAATELoAAABMQOgCAAAwAaELAADABIQuAAAAExC6AAAATEDoAgAAMMH/B5BQ4MdLXT5eAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(Actuals_e,Predictions_e)\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'energy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 81\u001b[0m\n\u001b[1;32m     78\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m dm\u001b[38;5;241m.\u001b[39mtest_dataloader()\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Process training data\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m train_results \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_data_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_factors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Process test data\u001b[39;00m\n\u001b[1;32m     84\u001b[0m test_results \u001b[38;5;241m=\u001b[39m process_data_loader(test_loader, norm_factors, task, limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 30\u001b[0m, in \u001b[0;36mprocess_data_loader\u001b[0;34m(data_loader, norm_factors, task, limit)\u001b[0m\n\u001b[1;32m     27\u001b[0m Result \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mforward(batch)\n\u001b[1;32m     29\u001b[0m Pred_Energy_corr \u001b[38;5;241m=\u001b[39m Result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression0\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrected_total_energy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m norm_factors[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menergy_std\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m norm_factors[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menergy_mean\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 30\u001b[0m Pred_Energy \u001b[38;5;241m=\u001b[39m \u001b[43mResult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgff_regression0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43menergy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m norm_factors[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menergy_std\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m norm_factors[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menergy_mean\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     32\u001b[0m Pred_Forces \u001b[38;5;241m=\u001b[39m Result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgff_regression0\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforce\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     33\u001b[0m Actual_Energy_corr \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menergy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'energy'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize lists for storing predictions and actual values\n",
    "def initialize_prediction_lists():\n",
    "    return {\n",
    "        'Predictions_corr_e': [],\n",
    "        'Actuals_corr_e': [],\n",
    "        'Predictions_e': [],\n",
    "        'Actuals_e': [],\n",
    "        'Predictions_Fx': [],\n",
    "        'Actuals_Fx': [],\n",
    "        'Predictions_Fy': [],\n",
    "        'Actuals_Fy': [],\n",
    "        'Predictions_Fz': [],\n",
    "        'Actuals_Fz': []\n",
    "    }\n",
    "\n",
    "# Function to process a data loader\n",
    "def process_data_loader(data_loader, norm_factors, task, limit=None):\n",
    "    results = initialize_prediction_lists()\n",
    "    counter = 0\n",
    "\n",
    "    for batch in tqdm(data_loader):\n",
    "        Result = task.forward(batch)\n",
    "\n",
    "        Pred_Energy_corr = Result['regression0']['corrected_total_energy'].item() * norm_factors['energy_std'] + norm_factors['energy_mean']\n",
    "        Pred_Energy = Result['force_regression0']['energy'].item() * norm_factors['energy_std'] + norm_factors['energy_mean']\n",
    "\n",
    "        Pred_Forces = Result['force_regression0']['force']\n",
    "        Actual_Energy_corr = batch['targets']['energy'].item()\n",
    "        Actual_Energy = batch['targets']['corrected_total_energy'].item()\n",
    "        \n",
    "        Actual_Forces = batch['targets']['force']\n",
    "\n",
    "        results['Predictions_corr_e'].append(Pred_Energy_corr)\n",
    "        results['Actuals_corr_e'].append(Actual_Energy_corr)\n",
    "        \n",
    "        results['Predictions_e'].append(Pred_Energy)\n",
    "        results['Actuals_e'].append(Actual_Energy)\n",
    "        \n",
    "        results['Predictions_Fx'] += Pred_Forces[:, 0].reshape(-1).detach().numpy().tolist()\n",
    "        results['Actuals_Fx'] += Actual_Forces[:, 0].reshape(-1).detach().numpy().tolist()\n",
    "\n",
    "        results['Predictions_Fy'] += Pred_Forces[:, 1].reshape(-1).detach().numpy().tolist()\n",
    "        results['Actuals_Fy'] += Actual_Forces[:, 1].reshape(-1).detach().numpy().tolist()\n",
    "\n",
    "        results['Predictions_Fz'] += Pred_Forces[:, 2].reshape(-1).detach().numpy().tolist()\n",
    "        results['Actuals_Fz'] += Actual_Forces[:, 2].reshape(-1).detach().numpy().tolist()\n",
    "\n",
    "        counter += 1\n",
    "        if limit and counter >= limit:\n",
    "            break\n",
    "\n",
    "    return results\n",
    "\n",
    "# Load data\n",
    "dm = MatSciMLDataModule(\n",
    "    \"MaterialsProjectDataset\",\n",
    "    train_path=\"/home/civil/phd/cez218288/scratch/Scaling_lmdb_new/10k_new\",\n",
    "    test_split=\"/home/civil/phd/cez218288/scratch/Scaling_lmdb_new/test_new\",\n",
    "    dset_kwargs={\n",
    "        \"transforms\": [\n",
    "            PeriodicPropertiesTransform(cutoff_radius=6.0, adaptive_cutoff=True),\n",
    "            PointCloudToGraphTransform(\n",
    "                \"pyg\",\n",
    "                node_keys=[\"pos\", \"atomic_numbers\"],\n",
    "            ),\n",
    "        ],\n",
    "    },\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "dm.setup()\n",
    "train_loader = dm.train_dataloader()\n",
    "test_loader = dm.test_dataloader()\n",
    "\n",
    "# Process training data\n",
    "train_results = process_data_loader(train_loader, norm_factors, task, limit=100)\n",
    "\n",
    "# Process test data\n",
    "test_results = process_data_loader(test_loader, norm_factors, task, limit=100)\n",
    "\n",
    "# Function to plot R² score with train and test data\n",
    "# Function to plot R² score with train and test data\n",
    "def plot_r2_score(train_actual, train_pred, test_actual, test_pred, title=\"Title\", suffix=\"10k_Faenet\"):\n",
    "    save_dir = '/home/civil/phd/cez218288/scratch/Simulation/plots/'  # Ensure this path ends with a slash\n",
    "\n",
    "    # Calculate R² score for train and test data\n",
    "    r2_train = r2_score(train_actual, train_pred)\n",
    "    r2_test = r2_score(test_actual, test_pred)\n",
    "\n",
    "    # Create the scatter plot\n",
    "    plt.figure(figsize=(6, 6))  # Adjust the figure size as needed\n",
    "    plt.scatter(train_actual, train_pred, label='Train', color='blue')\n",
    "    plt.scatter(test_actual, test_pred, label='Test', color='green')\n",
    "    plt.xlabel(\"Actual\", fontsize=20, fontweight='bold')\n",
    "    plt.ylabel(\"Predicted\", fontsize=20, fontweight='bold')\n",
    "    plt.xticks(rotation=90, fontsize=20, fontweight='bold')\n",
    "    plt.yticks(fontsize=20, fontweight='bold')\n",
    "    # Plot the 45-degree line\n",
    "    min_val = min(min(train_actual), min(test_actual), min(train_pred), min(test_pred))\n",
    "    max_val = max(max(train_actual), max(test_actual), max(train_pred), max(test_pred))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--')\n",
    "    plt.title(title, fontsize=20, fontweight='bold')\n",
    "\n",
    "    # Annotate the R² scores on the plot\n",
    "    plt.text(0.05, 0.95, f'Train R² = {r2_train:.5f}', transform=plt.gca().transAxes, fontsize=12, fontweight='bold', verticalalignment='top', color='blue')\n",
    "    plt.text(0.05, 0.90, f'Test R² = {r2_test:.5f}', transform=plt.gca().transAxes, fontsize=12, fontweight='bold', verticalalignment='top', color='green')\n",
    "\n",
    "    # Save the plot to the specified location with the title in the filename\n",
    "    filename = f\"{title.replace(' ', '_')}_{suffix}.png\"\n",
    "    plt.savefig(f'{save_dir}{filename}', bbox_inches='tight')  # Use bbox_inches='tight' to adjust the bounding box\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    # Clear the current figure to avoid overlap\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "# Plot and save the results for combined training and test data\n",
    "plot_r2_score(train_results['Actuals_corr_e'], train_results['Predictions_corr_e'], test_results['Actuals_corr_e'], test_results['Predictions_corr_e'], \"Corrected Total Energy\", \"10k_mace\")\n",
    "plot_r2_score(train_results['Actuals_e'], train_results['Predictions_e'], test_results['Actuals_e'], test_results['Predictions_e'], \"Energy\", \"10k_mace\")\n",
    "plot_r2_score(train_results['Actuals_Fx'], train_results['Predictions_Fx'], test_results['Actuals_Fx'], test_results['Predictions_Fx'], \"Fx\", \"10k_mace\")\n",
    "plot_r2_score(train_results['Actuals_Fy'], train_results['Predictions_Fy'], test_results['Actuals_Fy'], test_results['Predictions_Fy'], \"Fy\", \"10k_mace\")\n",
    "plot_r2_score(train_results['Actuals_Fz'], train_results['Predictions_Fz'], test_results['Actuals_Fz'], test_results['Predictions_Fz'], \"Fz\", \"10k_mace\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
